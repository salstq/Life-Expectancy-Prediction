# -*- coding: utf-8 -*-
"""Machine_Learning_Terapan_Salsa_Tashfiyatul_Qolbi_Submission_1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jrdSTIjQZK1ez8_cdJSkOqu3k-1AYe5W

# Predicitve Analytics : Life Expectancy Prediction

## Business Understanding
### Problem Statements
* Dapatkah umur harapan hidup suatu negara diprediksi secara akurat berdasarkan faktor-faktor sosial, ekonomi, dan kesehatan?
* Apa saja faktor yang paling berpengaruh terhadap harapan hidup suatu negara?

### Goals
* Mengidentifikasi fitur-fitur yang paling signifikan terhadap harapan hidup.
* Membangun model prediktif yang akurat untuk memperkirakan nilai harapan hidup.
* Membandingkan performa beberapa algoritma regresi dan memilih model terbaik.

### Solution Statements
* Menerapkan beberapa algoritma regresi, yaitu Linear Regresi, Random Forest, dan Gradient Boosting.
* Melakukan preprocessing data secara lengkap, seperti imputasi missing value, penanganan outlier, encoding, dan stadarisasi.
* Melakukan hyperparameter tuning pada model terbaik untuk meningkatkan performa.
* Menggunakan metrik MAE, MSE, RMSE, dan RÂ² untuk mengevaluasi model.

## Data Understanding
Dataset Life Expectancy (WHO) yang berasal dari Kaggle [Life Expectancy (WHO)](https://www.kaggle.com/datasets/kumarajarshi/life-expectancy-who). Dataset ini memiliki 2938 baris dan 22 kolom yang terdiri atas 20 kolom numerik dan 2 kolom kategorikal. Penjelasan lebih rinci akan dijelaskan dalam tahap berikut ini:

## Data Understanding

Dalam tahap ini, data akan diproses untuk memahami isi dari dataset.

### Data Loading

#### Import Library

Mengimport seluruh library yang dibutuhkan untuk analisis data dan pembangunan model machine learning.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer
import kagglehub
from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RandomizedSearchCV

"""Fungsi semua library yang diimport:
1. **import pandas as pd** : untuk manipulasi dan analisis data
2. **import numpy as np** : untuk operasi numerik dan array multidimensi
3. **import matplotlib.pyplot as plt** : untuk visualisasi data
4. **import seaborn as sns** : untuk visualisasi statistik
5. **from sklearn.preprocessing import LabelEncoder** : untuk mengubah fitur kategorikal menjadi numerikal
6. **from sklearn.impute import SimpleImputer** : untuk mengimputasi nilai yang hilang
7. **import kagglehub** : untuk mengakses dataset dari KaggleHub
8. **from sklearn.preprocessing import RobustScaler** : untuk normalisasi fitur numerik
9. **from sklearn.model_selection import train_test_split** : untuk membagi dataset menjadi data latih dan data uji
10. **from sklearn.linear_model import LinearRegression** : untuk model regresi linier yang memprediksi variabel kontinu
11. **from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor** : untuk mode regresi ensemble berbasis randomforest dan regresi berbasis boosting
12. **from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score** : untuk mengevaluasi model dengan MAE, MSE, dan R^2
13. **from sklearn.model_selection import cross_val_score** : untuk evaluasi model dengan cross-validation
14. **from sklearn.model_selection import RandomizedSearchCV** : untuk tuning hyperparameter dengan pencarian acak menggunakan cross-validation

#### Memuat Dataset

Pada tahap ini memuat dataset ke dalam notebook. Karena dataset memiliki format CSV, maka menggunakan library pandas untuk membacanya.
"""

path = kagglehub.dataset_download("kumarajarshi/life-expectancy-who")
file_path = path + "/Life Expectancy Data.csv"
data = pd.read_csv(file_path)

"""### Exploratory Data Analysis (EDA)

Analisis karakteristik, menemukan pola, anomali, dan memeriksa asumsi pada data.

#### Cek Tipe Data
"""

data.info()

"""Terlihat bahwa dataset memiliki 2938 baris dan 22 kolom yang terdiri dari 2 kolom data kategorikal dan 20 kolom data numerik. Berikut adalah seluruh fitur yang ada dalam dataset:
1. Country: berisi 193 negara
2. Year: tahun pengambilan data
3. Status: berisi status dari sebuha negara, negara berkembang atau negara maju
4. Life Expectancy: harapan hidup berdasarkan umur
5. Adult Mortality: kematian orang dewasa per 1000 populasi
6. Infant Deaths: kematian bayi per 1000 populasi
7. Alcohol: konsumsi alcohol per kapita
8. Percentage Expenditure: persentase pengeluaran untuk Kesehatan pada produk domestic bruto per kapita
9. Hepatitis B: cakupan imunisasi pada anak usia 1 tahun
10. Measles: kasus campak yang tercatat per 1000 populasi
11. BMI: rata-rata indeks massa tubuh seluruh populasi
12. Under-five deaths: angka kematian anak dibawah 5 tahun per 1000 kematian
13. Polio: cakupan imunisasi polio pada anak umur 1 tahun
14. Total expenditure: pengeluaran pemerintah umum untuk Kesehatan sebagai persentase dari total pengeluaran pemerintah
15. Diphtheria: cakupan imunisasi Diphtheria tetanus toxoid dan pertussis (DTP3) pada anak umur 1 tahun
16. HIV/AIDS: kematian per 1000 kelahiran hidup anak dengan HIV/AIDS (0-4 tahun)
17. GDP: produk domestic bruto per kapita di USD
18. Population: populasi dari negara
19. Thinness 10-19 years: prevalensi kurus pada anak dan remaja usia 10-19 tahun
20. Thinness 5-9 years: prevalensi kurus pada anak dan remaja usia 5-9 tahun
21. Income composition: Indeks Pembangunan Manusia dalam hal komposisi pendapatan sumber daya (indeks berkisar antara 0 sampai 1)
22. Schooling: Jumlah Tahun Sekolah (tahun)


Dengan tipe data sebagai berikut:
* Terdapat 2 tipe data object
* Terdapat 4 tipe data int64
* Terdapat 16 tipe data float64

#### Ganti Nama Kolom yang Salah/Tidak Rapih
"""

data.rename(columns={' thinness  1-19 years': 'thinness 10-19 years', 'Life expectancy ': 'Life expectancy', 'Measless ': 'Measless', ' BMI ': 'BMI', 'under-five deaths ': 'under-five deaths', 'Diphtheria ': 'Diphtheria', ' HIV/AIDS':'HIV/AIDS', ' thinness 5-9 years':'thinness 5-9 years'}, inplace=True)

"""Terdapat kesalahan penamaan pada kolom thinness  1-19 years, isi dari kolom tersebut adalah data orang dari umur 10-19 tahun, sehingga kolom itu harus diganti namanya. Selain itu, terdapat beberapa kolom yang penamaannya tidak rapih.

#### Cek Isi Baris
"""

data.head()

"""#### Cek Deskripsi Statistik"""

data.describe(include='all')

"""Fungsi describe digunakan untuk memberikan informasi statistik.

* Count adalah jumlah sampel pada data.
* Mean adalah nilai rata-rata.
* Std adalah standar deviasi.
* Min yaitu nilai minimum.
* 25% adalah kuartil pertama.
* 50% adalah kuartil kedua.
* 75% adalah kuartil ketiga.
* Max adalah nilai maksimum.

#### Cek Data Hilang dan Penanganannya
"""

missing_values = data.isna().sum()
print(missing_values)

"""Terlihat bahwa dalam dataset terdapat missing value. Karena data yang hilang semuanya berada pada kolom numerik, maka solusi yang dapat dilakukan adalah dengan imputasi atau mengisi missing value dengan median."""

# Imputasi semua missing values numerik kontinu dengan median
data.fillna(data.median(numeric_only=True), inplace=True)
print(data.isnull().sum())

"""Setelah imputasi dilakukan, sudah tidak ada lagi data yang hilang.

#### Cek Duplikasi Data
"""

duplicates = data.duplicated()
print("Baris duplikat:", duplicates.sum())

"""Tidak terdapat data yang terduplikasi

### EDA - Univariate Analysis
"""

numeric_features = data.select_dtypes(include=['number']).columns
plt.figure(figsize=(12, 100))
for i, col in enumerate(numeric_features, 1):
    plt.subplot(20, 1, i)
    sns.histplot(data[col], bins=30, kde=True)
    plt.title(f"Distribusi {col}")
plt.tight_layout()
plt.show()

categorical_features = ['Status']
plt.figure(figsize=(12, 8))
for i, col in enumerate(categorical_features, 1):
    plt.subplot(2, 1, i)
    sns.countplot(x=data[col])
    plt.title(f"Distribusi {col}")
    plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""Pada distribusi data kategorikal hanya menampilkan kolom status, karena kolom country memiliki banyak jenis yang membuatnya tidak memungkinkan untuk ditampilkan dalam bentuk subplot."""

print("Jumlah Negara:", data['Country'].nunique())
country_counts = data['Country'].value_counts()
print(country_counts)

"""Berikut adalah distribusi dari kolom Country, terdapat 193 negara di dalamnya.

### Correlation Matrix

Digunakan untuk melihat fitur apa saja yang memiliki korelasi.
"""

correlation = data[numeric_features].corr()
plt.figure(figsize=(18, 12))
sns.heatmap(correlation, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Heatmap Korelasi Antar Variabel Numerik")
plt.show()

"""Terdapat beberapa kolom numerik yang memiliki korelasi yang cukup tinggi, seperti kolom GDP dengan kolom percentage expenditure. Kolom-kolom yang memiliki korelasi tinggi, akan dihapus salah satunya untuk menghindari bias pada data.

## Data Preparation

Teknik yang akan dilakukan:
* Fitur selection : menghapus beberapa fitur yang tidak relevan dan berkorelasi tinggi
* Outliers checking & handling : Mengurangi outliers
* Standarisasi : Menyamakan skala fitur
* Label encoding : Mengubah fitur kategorikal menjadi numerik
* Split data : Membagi dataset menjadi 2, yaitu data latih (70%) dan data uji (20%)

### Fitur Selection
"""

print(data.columns.tolist())

# Contoh drop kolom dengan korelasi sangat tinggi
data = data.drop(['infant deaths', 'percentage expenditure', 'Country', 'thinness 10-19 years', 'Hepatitis B'], axis=1)

"""### Outlier Checking & Handling

Boxplot sebelum IQR Method
"""

num_cols = data.select_dtypes(include=['number']).columns
for feature in num_cols:
    plt.figure(figsize=(10, 6))
    sns.boxplot(x=data[feature])
    plt.title(f'Box Plot of {feature}')
    plt.show()

"""Terdapat beberapa kolom dengan outlier tinggi."""

# Contoh sederhana untuk mengidentifikasi outliers menggunakan IQR
Q1 = data[num_cols].quantile(0.25)
Q3 = data[num_cols].quantile(0.75)
IQR = Q3 - Q1

# Filter dataframe untuk hanya menyimpan baris yang tidak mengandung outliers pada kolom numerik
condition = ~((data[num_cols] < (Q1 - 1.5 * IQR)) | (data[num_cols] > (Q3 + 1.5 * IQR))).any(axis=1)
data_filtered_numeric = data.loc[condition, num_cols]

# Menggabungkan kembali dengan kolom kategorikal
df = pd.concat([data_filtered_numeric, data.loc[condition, categorical_features]], axis=1)

"""Tahap ini dilakukan dengan tujuan untuk menghapus outliers atau data-data yang berada di luar IQR. Penghapusan dilakukan agar hasil model tidak terpengaruh oleh outliers.

`num_cols` adalah variabel yang memanggil fitur-fitur numerik dari dataset.
```
Q1 = data[num_cols].quantile(0.25)
Q3 = data[num_cols].quantile(0.75)
IQR = Q3 - Q1
```
* Q1 (Kuartil 1): Nilai yang memisahkan 25% data terendah dari 75% data lainnya.
* Q3 (Kuartil 3): Nilai yang memisahkan 75% data terendah dari 25% data tertinggi.
* IQR (Interquartile Range): Rentang antara Q3 dan Q1. Ini mengukur sebaran nilai tengah dari data.

Boxplot setelah IQR Method
"""

for feature in num_cols:
    plt.figure(figsize=(10, 6))
    sns.boxplot(x=df[feature])
    plt.title(f'Box Plot of {feature}')
    plt.show()

print("Data awal:", data.shape)
print("Data setelah filter outlier:", df.shape)

"""Setelah penanganan oulier terhadap kolom numerik, data berkurang menjadi 1149 baris."""

df.info()

"""### Standarisasi

Standarisasi dilakukan dengan menggunakan Robust Scaler
"""

target_col = 'Life expectancy'
new_data_robust = df.copy()
num_cols_wo_target = [col for col in num_cols if col != target_col]
scaler = RobustScaler()
new_data_robust[num_cols_wo_target] = scaler.fit_transform(df[num_cols_wo_target])

"""Penggunaan Robust Scaler bertujuan untuk menstandarisasi fitur numerik agar memiliki nilai yang sama. Robust Scaler menggunakan median dan IQR Method untuk menstandarisasi nilai. Dengan menggunakan Robust Scaler, hasil standarisasi lebih tahan terhadap outlier."""

# Histogram Sebelum Standardisasi
plt.figure(figsize=(12, 100))
for i, col in enumerate(num_cols, 1):
    plt.subplot(18, 1, i)
    sns.histplot(df[col], kde=True)
    plt.title(f"Histogram Sebelum Standardisasi {col}")
plt.suptitle("Histogram Sebelum Standardisasi", fontsize=30, y=1.02)
plt.tight_layout()
plt.show()

# Histogram Sesudah Standardisasi
plt.figure(figsize=(12, 100))
for i, col in enumerate(num_cols, 1):
    plt.subplot(18, 1, i)
    sns.histplot(new_data_robust[col], kde=True)
    plt.title(f"Histogram Sesudah Standardisasi {col}")
plt.suptitle("Histogram Sesudah Standardisasi", fontsize=30, y=1.02)
plt.tight_layout()
plt.show()

"""### Label **Encoding**

Mengubah kolom kategorikal menjadi kolom numerik.
"""

new_data_robust[categorical_features]

new_one_hot_encoded = pd.get_dummies(new_data_robust[categorical_features]).astype(int)
new_data_wo_nominal = new_data_robust.drop(columns=categorical_features)
new_data = pd.concat([new_data_wo_nominal, new_one_hot_encoded], axis=1)
new_data.head()

"""##### Data Bersih

Berikut adalah info dari data yang sudah bersih.
"""

new_data.info()

"""### Split Data"""

y = new_data['Life expectancy']
X = new_data.drop(columns=['Life expectancy'])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

"""Membagi data menjadi 2 untuk fitur menjadi variabel X dan label menjadi variabel y, dengan ukuran 70% data uji dan 30% data tes.

"""

X

y

"""## Data Modelling

Menggunakan tiga model, yaitu:
* Linear Regression
* Random Forest Regressor
* Gradient Boosting Regressor

### LinearRegression
"""

lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
lr_pred = lr_model.predict(X_test)

"""Linear Regression adalah model paling sederhana karena memetakan hubungan linier antara input (X) dengan target (y) dengan persamaan garis lurus.

Kelebihan:
* Sederhana dan cepat
* Bisa digunakan sebagai baseline model

Kekurangan:
* Tidak cocok untuk hubungan non-linier
* Sangat sensitif dengan outliers

### RandomForestRegressor
"""

rf_model = RandomForestRegressor(random_state=42)
rf_model.fit(X_train, y_train)
rf_pred = rf_model.predict(X_test)

"""Random Forest Regressor adalah ensemble model berbasis pohon keputusan. Algoritma ini membangun banyak pohon keputusan secara acak pada subset data, kemudian menggabungkan hasil prediksi dari masing-masing pohon dengan metode voting

Kelebihan:
* Kuat terhadap overfitting
* Tidak sensitif terhadap outliers

Kekurangan:
* Lebih lambat dari Linear Regression
* Butuh tuning parameter untuk performa maksimal

### GradientBoostingRegressor
"""

gb_model = GradientBoostingRegressor(random_state=42)
gb_model.fit(X_train, y_train)
gb_pred = gb_model.predict(X_test)

"""Gradient Boosting Regressor adalah model ensemble lain berbasis decision tree, tapi menggunakan pendekatan boosting, artinya membangun pohon bertahap, di mana tiap pohon mencoba memperbaiki error dari pohon sebelumnya.

Kelebihan:
* Akurasi tinggi
* Bisa menangani berbagai jenis data

Kekurangan:
* Rentan overfitting
* Butuh waktu lebih lama

## Evaluasi

Menggunakan metrik evaluasi:
* MAE : Mengukur rata-rata selisih absolut antara nilai prediksi dan nilai asli.
* MSE : Mengukur rata-rata kuadrat selisih antara nilai prediksi dan nilai asli.
* RMSE :  Akar dari MSE, menunjukkan seberapa jauh prediksi dari nilai aslinya dalam satuan aslinya.
* RÂ² : Mengukur seberapa baik variabel independen menjelaskan variasi target (0â1).
* Cross-Validation : Membagi data menjadi beberapa subset (fold) untuk melatih dan menguji model secara bergantian.
"""

def evaluate(y_true, y_pred, model_name):
    print(f"{model_name}")
    print("MAE:", mean_absolute_error(y_true, y_pred))
    print("MSE:", mean_squared_error(y_true, y_pred))
    print("RMSE:", np.sqrt(mean_squared_error(y_true, y_pred)))
    print("RÂ² Score:", r2_score(y_true, y_pred))
    print("="*40)

evaluate(y_test, lr_pred, "Linear Regression")
evaluate(y_test, rf_pred, "Random Forest")
evaluate(y_test, gb_pred, "Gradient Boosting")

"""Dari hasil evaluasi, terlihat bahwa model RandomForest menjadi model terbaik untuk melatih data.

### Cross-Validation
"""

models = {
    "Linear Regression": lr_model,
    "Random Forest": rf_model,
    "Gradient Boosting": gb_model
}

for name, model in models.items():
    scores = cross_val_score(model, X, y, cv=10, scoring='r2')
    print(f"{name}")
    print("R2 Scores per fold:", scores)
    print("Mean R2 Score:", scores.mean())
    print("="*40)

"""Evaluasi menggunakan cross-validation memperlihatkan jika model RandomForest tetap model terbaik untuk melatih data. Karena mode Random Forest menjadi model yang paling baik, maka akan dilakukan optimisasi dengan Hyperparameter Tuning menggunakan RandomizedSearchCV agara hasilnya optimal.

## Hyperparameter Tuning

* Feature Importance
* Hyperparameter Tuning
* Evaluasi
* Visualisasi

### Feature Importance

Memilih fitur yang paling penting pada algoritma Random Forest. Memilih 10 fitur tertinggi.
"""

# Cek feature importances
importances = rf_model.feature_importances_
feature_names = X_train.columns if hasattr(X_train, 'columns') else [f'Feature {i}' for i in range(len(importances))]

# Buat plot
plt.figure(figsize=(10, 6))
sorted_idx = np.argsort(importances)
plt.barh(range(len(importances)), importances[sorted_idx], align='center')
plt.yticks(range(len(importances)), [feature_names[i] for i in sorted_idx])
plt.xlabel('Feature Importance')
plt.title('Feature Importance (Random Forest)')
plt.tight_layout()
plt.show()

"""### Hyperparameter Tuning"""

# Model dasar
rf = RandomForestRegressor(random_state=42)

# Grid parameter yang akan di-random search
param_distributions = {
    'n_estimators': [50, 100, 150],
    'max_depth': [5, 8, 10],
    'min_samples_split': [5, 10],
    'min_samples_leaf': [4, 6, 8],
    'max_features': ['sqrt'],
    'bootstrap': [True]
}


# Randomized search
random_search = RandomizedSearchCV(
    estimator=rf,
    param_distributions=param_distributions,
    n_iter=50,
    cv=5,
    scoring='r2',
    n_jobs=-1,
    verbose=2,
    random_state=42
)

important_features = [
    'Income composition of resources', 'Adult Mortality', 'thinness 5-9 years',
    'Total expenditure', 'Alcohol', 'Population',
    'Schooling', 'GDP', 'under-five deaths', 'BMI'
]

X_train_selected = X_train[important_features]
X_test_selected = X_test[important_features]

# Fit model
random_search.fit(X_train_selected, y_train)

# Evaluasi
best_model = random_search.best_estimator_
y_pred = best_model.predict(X_test_selected)

mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print("MAE:", mae)
print("MSE:", mse)
print("RMSE:", rmse)
print("RÂ² Score:", r2)

"""RandomizedSearchCV digunakan untuk mencari kombinasi parameter terbaik secara acak dari:
* n_estimators: Jumlah pohon dalam hutan
* max_depth: Kedalaman maksimum tiap pohon
* min_samples_split: Minimum sampel untuk membagi node
* min_samples_leaf: Minimum sampel pada daun
* max_features: Jumlah fitur yang dipertimbangkan di setiap split
* bootstrap: Apakah memakai bootstrapped samples atau tidak

`cv=5` artinya 5-fold cross-validation dipakai untuk mengevaluasi performa kombinasi parameter secara lebih baik.

### Evaluasi
"""

# Prediksi untuk train dan test
y_train_pred = best_model.predict(X_train_selected)
y_test_pred = best_model.predict(X_test_selected)

# Evaluasi Train
mae_train = mean_absolute_error(y_train, y_train_pred)
mse_train = mean_squared_error(y_train, y_train_pred)
rmse_train = np.sqrt(mse_train)
r2_train = r2_score(y_train, y_train_pred)

# Evaluasi Test
mae_test = mean_absolute_error(y_test, y_test_pred)
mse_test = mean_squared_error(y_test, y_test_pred)
rmse_test = np.sqrt(mse_test)
r2_test = r2_score(y_test, y_test_pred)

# Cetak hasil
print("=== Train Set Evaluation ===")
print("MAE:", mae_train)
print("MSE:", mse_train)
print("RMSE:", rmse_train)
print("RÂ² Score:", r2_train)

print("\n=== Test Set Evaluation ===")
print("MAE:", mae_test)
print("MSE:", mse_test)
print("RMSE:", rmse_test)
print("RÂ² Score:", r2_test)

"""* Model tidak overfitting: RÂ² pada train = 0.93 dan test = 0.88, jadi performanya stabil di kedua dataset.

* Perbedaan MAE dan RMSE antara train dan test tidak terlalu besar, yang berarti model berhasil belajar pola tanpa terlalu menghafal data.

* Random Forest Regressor dengan tuning + feature selection memberikan hasil prediksi umur harapan hidup yang akurat.

### Visualisasi
"""

plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_test_pred, alpha=0.7, color='blue')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Actual vs Predicted (Test Data)')
plt.grid(True)
plt.show()

"""## Dampak Model terhadap Business Understanding
### Apakah Model Menjawab Problem Statements?
* **Ya**, model berhasil membangun prediksi umur harapan hidup menggunakan beberapa algoritma regresi, dan berhasil mengidentifikasi faktor-faktor penting, seperti fitur importance yang telah dissebutkan di atas.

### Apakah Model Berhasil Mencapai Goals?
* **Ya**, model berhasil mengidentifikasi fitur-fitur yang signifikan dalam memprediksi harapan hidup, dan model mampu membangun model yang dapat memprediksi dengan akurat. Penggunaan tiga jenis algoritma ini memungkinkan untuk dilakukan perbandingan terhadap performa masing-masing model, dan didapat model terbaik, yaitu Random Forest Regressor.

### Apakah Solusi yang Direncanakan Berdampak?
* **Ya**, model ini berdampak, karena memeberikan prediksi akurat untuk umur harapan hidup, dapat membantu pemngambil kebijakan memahami faktor utama yang mempengaruhi harapan hidup, dan hasil dapat digunakan untuk intervensi kebijakan berbasis data di bidang kesehatan dan ekonomi.

"""